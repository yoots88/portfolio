{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project: Dataset Name - Exploratory Data Analysis ##\n",
    "\n",
    "\n",
    "**Problem Statement:**  \n",
    "\n",
    "**Stakeholders:** \n",
    "* Group 1\n",
    "* Group 2\n",
    "* Group 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A. Import Cleaned Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries #\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Authorization #\n",
    "__author__ = \"Taesun Yoo\"\n",
    "__email__ = \"yoots1988@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Write Out List of Functions --- #\n",
    "def load_file(file):\n",
    "    '''load input CSVs as a dataframe'''\n",
    "    return pd.read_csv(file, encoding='latin1')\n",
    "\n",
    "\n",
    "def convert_dt_as_date(df, var_name, date_format):\n",
    "    '''convert the variable as specified date format'''\n",
    "    df[var_name] = pd.to_datetime(df[var_name], format=date_format)\n",
    "    return df[var_name]\n",
    "\n",
    "\n",
    "def convert_dt_as_custom(df, var_name, dt_type):\n",
    "    '''convert datatype on selected variables'''\n",
    "    df[var_name] = df[var_name].astype(dt_type)\n",
    "    return df[var_name]\n",
    "\n",
    "\n",
    "def convert_dt_to_cat(df):\n",
    "    '''convert data type to category'''\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "\n",
    "def eda_encode_cat_var(df, col, num_var):\n",
    "    '''encode the cat. variables by mean of a num. variable by each cat'''\n",
    "    cat_dict={}\n",
    "    cats = df[col].cat.categories.tolist()\n",
    "    for cat in cats:\n",
    "        cat_dict[cat] = df[df[col] == cat][num_var].mean()\n",
    "    df[col] = df[col].map(cat_dict)\n",
    "\n",
    "\n",
    "#def pandas_qcut(df, new_var, old_var, q):\n",
    "#    ''' apply pd.qcut for equal size binning '''\n",
    "#    df[new_var] = pd.qcut(df[old_var], q=q, duplicates='drop',\n",
    "#                          precision=0, labels=None)\n",
    "#    return df[new_var]\n",
    "\n",
    "\n",
    "def estimate_width(df, var_name, interval_size):\n",
    "    ''' estimate the width of continuous variable '''\n",
    "    width = (max(df[var_name]) - min(df[var_name]))/interval_size\n",
    "    df_estimate = pd.DataFrame([[min(df[var_name])-1, max(df[var_name])+1, round(width)]])\n",
    "    df_estimate.columns = ['min', 'max', 'width']\n",
    "    return df_estimate\n",
    "\n",
    "\n",
    "def pandas_cut(df, new_var, old_var, bins, labels=None):\n",
    "    ''' apply pd.cut for custom size binning '''\n",
    "    df[new_var] = pd.cut(df[old_var], bins=bins, \n",
    "                          labels=labels, right=False)\n",
    "    return df[new_var]\n",
    "\n",
    "\n",
    "def check_value_counts(df, var_name):\n",
    "    '''return grouped value counts'''\n",
    "    grouped_counts = df[var_name].value_counts()\n",
    "    return grouped_counts\n",
    "\n",
    "\n",
    "def feature_replacement(X):\n",
    "    ''' replace missing values based on specific data type of a column '''\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype.name == 'object':\n",
    "            mode = X[col].mode().iloc[0]\n",
    "            X[col] = X[col].fillna(mode)\n",
    "        elif X[col].dtype.name == 'float64':\n",
    "            mean = X[col].mean()\n",
    "            X[col] = X[col].fillna(mean)\n",
    "        else:\n",
    "            X[col].dtype.name == 'int64'\n",
    "            median = X[col].median()\n",
    "            X[col] = X[col].fillna(median)\n",
    "            \n",
    "\n",
    "def eda_missing(df):\n",
    "    '''compute missing % on each var'''\n",
    "    df_missing = pd.DataFrame(df.isnull().sum(), columns=['count'])\n",
    "    df_missing['pct'] = (df_missing['count']/len(df)) * 100\n",
    "    return df_missing\n",
    "\n",
    "\n",
    "def eda_stat_num(df):\n",
    "    ''' perform eda for numerical features '''\n",
    "    df_stat_num = df.describe().T\n",
    "    df_stat_num = df_stat_num[['count', 'min', 'mean', 'max', 'std', '25%', '50%', '75%']]\n",
    "    df_stat_num = pd.DataFrame(df_stat_num)\n",
    "    return df_stat_num\n",
    "\n",
    "\n",
    "def eda_stat_cat(df):\n",
    "    ''' perform eda for categorical features '''\n",
    "    df_stat_cat = df.describe(include='category').T\n",
    "    df_stat_cat = pd.DataFrame(df_stat_cat)\n",
    "    return df_stat_cat\n",
    "\n",
    "\n",
    "def eda_outliers(df):\n",
    "    '''check outliers using the IQR method'''\n",
    "    df['IQR'] = df['75%'] - df['25%']\n",
    "    df['LB']  = df['25%'] - 1.5*df['IQR']\n",
    "    df['UB']  = df['75%'] + 1.5*df['IQR']\n",
    "    df = df.drop(['count','std','mean','25%','50%','75%','IQR'], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def eda_agg_df_var(df, cat_var, kpi_dict):\n",
    "    '''compute aggregated dataframe to calculate the KPIs'''\n",
    "    df_agg = df.groupby(by=cat_var).agg(kpi_dict)\n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def eda_grouped_df_var(df, cat_var):\n",
    "    '''create a grouped dataframe by categorical variable'''\n",
    "    df_grp = pd.DataFrame(df.groupby([cat_var])[cat_var].count())\n",
    "    df_grp.columns = ['count']\n",
    "    return df_grp\n",
    "\n",
    "\n",
    "def plot_hist(df, var_1):\n",
    "    '''plot a histogram'''\n",
    "    plt.figure()\n",
    "    print(\"skenewss is:\", df[var_1].skew())\n",
    "    df[var_1].hist(color='green')\n",
    "    plt.title('Histogram of ' + var_1)\n",
    "    plt.xlabel(var_1)\n",
    "    plt.ylabel('patients')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bar_chart(df, var_name_1):\n",
    "    '''plot a bar chart'''\n",
    "    plt.figure()\n",
    "    var_count_1 = df[var_name_1].value_counts()\n",
    "    sns.barplot(var_count_1.index,  var_count_1.values, alpha=0.9)\n",
    "    plt.title('Frequency chart of ' + var_name_1)\n",
    "    plt.ylabel('patients')\n",
    "    plt.xlabel(var_name_1)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_freq_chart(x,y,df,order):\n",
    "    '''plot a frequency chart'''\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.countplot(x=x, hue=y, data=df, order=order)\n",
    "    plt.title('Bar chart: ' + x + ' of client group labels', fontsize=20)\n",
    "    plt.xticks(rotation=270, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xlabel(x, fontsize=12)\n",
    "    plt.ylabel('patients', fontsize=12)\n",
    "    plt.legend(loc='upper right', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pie_chart(df_1, var_name_1,\n",
    "                   df_2, var_name_2):\n",
    "    '''plot a pie chart of specified variables'''\n",
    "    plt.figure(figsize=(15,15))\n",
    "    # Sub-plot 1:\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.pie(df_1, autopct='%.0f%%', wedgeprops={'edgecolor':'white'},\n",
    "            textprops={'fontsize':15})\n",
    "    plt.title('Pie Chart of '+ var_name_1)\n",
    "    plt.legend(labels = df_1.index, loc='upper right')\n",
    "    # Sub-plot 2:\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.pie(df_2, autopct='%.0f%%', wedgeprops={'edgecolor':'white'},\n",
    "            textprops={'fontsize':15})\n",
    "    plt.title('Pie Chart of '+ var_name_2)\n",
    "    plt.legend(labels = df_2.index, loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_box(df, num_var_1, cat_var_1,\n",
    "             num_var_2, cat_var_2, \n",
    "             num_var_3, cat_var_3, hue=None):\n",
    "    '''plot a box-whisker of specified variables'''\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    # Sub-plot 1:\n",
    "    plt.subplot(1,3,1)\n",
    "    df.sort_values(by=[num_var_1], inplace=True)\n",
    "    sns.set(style='whitegrid')\n",
    "    sns.boxplot(cat_var_1, num_var_1, hue, df)\n",
    "    plt.title('Box plot of ' + num_var_1 + ' by ' + cat_var_1)\n",
    "    plt.xticks(rotation=270, fontsize=10)\n",
    "    # Sub-plot 2:\n",
    "    plt.subplot(1,3,2)\n",
    "    df.sort_values(by=[num_var_2], inplace=True)\n",
    "    sns.set(style='whitegrid')\n",
    "    sns.boxplot(cat_var_2, num_var_2, hue, df)\n",
    "    plt.title('Box plot of ' + num_var_2 + ' by ' + cat_var_2)\n",
    "    plt.xticks(rotation=270, fontsize=10)\n",
    "    # Sub-plot 3:\n",
    "    plt.subplot(1,3,3)\n",
    "    df.sort_values(by=[num_var_3], inplace=True)\n",
    "    sns.set(style='whitegrid')\n",
    "    sns.boxplot(cat_var_3, num_var_3, hue, df)\n",
    "    plt.title('Box plot of ' + num_var_3 + ' by ' + cat_var_3)\n",
    "    plt.xticks(rotation=270, fontsize=10)    \n",
    "\n",
    "    \n",
    "def plot_crosstab(df, cat_var_1, cat_var_2):\n",
    "    '''plot a crosstab of two categorical variables'''\n",
    "    table = pd.crosstab(df[cat_var_1], df[cat_var_2])\n",
    "    return table\n",
    "\n",
    "\n",
    "def plot_corr_matrix(df, list_vars):\n",
    "    ''' plot a correlation matrix '''\n",
    "    corr = df[list_vars].corr()\n",
    "    # Create a mask\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(corr)] = True\n",
    "    plt.figure(figsize=(12,12))\n",
    "    sns.heatmap(corr, mask=mask, square=True, linewidths = .5,\n",
    "                cmap=sns.diverging_palette(220,10,as_cmap=True),\n",
    "                vmin = -1, vmax = 1, fmt=\".2f\",\n",
    "                annot=True, annot_kws={'size':15})\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_scatter(df, var_1, var_2, color, factor=None):\n",
    "    '''Scatter plot of two continuous numeric features'''\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(df[var_1], df[var_2], color=color)\n",
    "    plt.title('Relationship between '+ var_1 + ' and ' + var_2)\n",
    "    plt.xlabel(var_1)\n",
    "    plt.ylabel(var_2)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def compute_pearson_r(df, var_x, var_y):\n",
    "    '''compute Pearson r correlation'''\n",
    "    corr_mat = np.corrcoef(df[var_x],df[var_y])\n",
    "    return corr_mat[0, 1]\n",
    "\n",
    "\n",
    "def plot_linear_reg(df, var_x, var_y, \n",
    "                    pearson_r, color, label):\n",
    "    '''plot a pair of linear regressions'''\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(df[var_x], df[var_y], 'r--', label='pearson_r =%.2f' % pearson_r,\n",
    "             marker='.', linestyle='none', color=color)\n",
    "    plt.margins(0.02)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel(var_x)\n",
    "    plt.ylabel(var_y)\n",
    "    plt.title(var_x + ' vs. ' + var_y + ' by ' + label)\n",
    "    # Fit linear regression:\n",
    "    a,b = np.polyfit(df[var_x], df[var_y], 1)\n",
    "    x = np.array([min(df[var_x]), max(df[var_x])])\n",
    "    y = a*x + b\n",
    "    plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'insurance_train.csv' does not exist: b'insurance_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-daabb08ab1a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# load data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf_eda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meda_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# map column names to lowercase:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-65c5550faabc>\u001b[0m in \u001b[0;36mload_file\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m'''load input CSVs as a dataframe'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'insurance_train.csv' does not exist: b'insurance_train.csv'"
     ]
    }
   ],
   "source": [
    "# --- 2. Load the data --- #\n",
    "# define input CSVs:\n",
    "if __name__ == '__main__':\n",
    "    eda_file = 'insurance_train.csv'\n",
    "\n",
    "# load data:\n",
    "df_eda = load_file(eda_file)\n",
    "\n",
    "# map column names to lowercase:\n",
    "df_eda.columns = map(str.lower, df_eda.columns)\n",
    "\n",
    "# check data types:\n",
    "df_eda.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables:\n",
    "#vars_cat = list(df_eda.select_dtypes(include='object').columns)\n",
    "#vars_num_disc = list(df_eda.select_dtypes(include='int64'))\n",
    "#vars_num_cont = list(df_eda.select_dtypes(include='float64'))\n",
    "\n",
    "# concatenate a list:\n",
    "#vars_num = vars_num_disc + vars_num_cont\n",
    "\n",
    "# delete a list of unwanted variables:\n",
    "#unwanted_list = {''}\n",
    "#vars_num = [var for var in vars_num if var not in unwanted_list]\n",
    "\n",
    "#var_label = \n",
    "#var_id = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Exploratory Data Analysis ##\n",
    "\n",
    "### Dataset Name: Training Set ###\n",
    "Data exploration is conducted on a cleaned training set. The main goal of this phase is to explore any interesting relationships among features and identify which features are good predictors on poverty rate predictions.\n",
    "\n",
    "Following set of questions are asked:\n",
    "1. Can I count something interesting?\n",
    "2. Can I find some trends (increase or decrease and any anomalies)?\n",
    "3. Can I plot a bar chart or a histogram?\n",
    "4. Can I make a scatter plot?\n",
    "\n",
    "These set of guiding questions will help us to explore any insights and tell a compelling story about the US poverty dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute top 10 rows:\n",
    "\n",
    "# check duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a missing % dataframe:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Missing Values % ###\n",
    "Above table shows that missing value % on each feature. app_underwriting_score misses about 3.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature impuation by data types:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a missing % dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data type: object to category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical features:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Numerical Features ###\n",
    "* age: mean is around 51\n",
    "* premium: mean is around ~ 11K\n",
    "* income: mean is around ~209K\n",
    "* % premium paid by cash credit: mean is 31%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Categorical Features ###\n",
    "* sourcing channel: majority of clients are sourced from channel A\n",
    "* residence area type: majority of clients are from Urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---4 detect outliers ---\n",
    "# create a dataframe for IQR:\n",
    "\n",
    "\n",
    "# check outliers:\n",
    "# lower bounds (LBs)\n",
    "\n",
    "# upper bounds (UBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---5 aggregate dataframe and compute KPIs ---\n",
    "# Define the dictionary for KPIs:\n",
    "\n",
    "\n",
    "# Print a summary KPI table by Renewal Status:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Renewal Status ###\n",
    "Comparison between non-renewed vs. renewed client groups:\n",
    "* mean age of non-renewed client group is younger and % of premium paid by cash credit is higher. \n",
    "* Also average income and average premium paid amount are lower than renewed client group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni-variate: Histogram Plots ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram:    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Histograms ###\n",
    "* Age: close to normal distribution with mean is around 51.\n",
    "* Premium: positively skewed with majority of clients paid premium lower than 15K.\n",
    "* % Premium Paid by Cash Credit: positively skewed with majority of clients less than 15%\n",
    "* Underwriting Score: negatively skewed with majority of clients above score 99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni-variate: Bar Charts ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a bar chart:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Bar Charts ###\n",
    "* Sourcing Channel: majority of clients sourced from Channel A followed by B, C, D and E.\n",
    "* Residence Area Type: majority of clients reside in Urban area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Charts ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a frequency chart:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Frequency Chart 1 ###\n",
    "* Age Group: majority of client age group is around 40 to 60s.\n",
    "* Income Group: majority of client income group is around 204K.\n",
    "* Premium Group: majority of client premium paid group is around 16K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni-variate: Pie Charts ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped dataframe by a categorical feature:\n",
    "\n",
    "\n",
    "# Plot pie chart(s) by categorical features:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Pie Chart ###\n",
    "* Renewal Status: 94 % of clients renewed their policies.\n",
    "* Residence Type: 60% of clients reside in Urban area.\n",
    "* Age Group: about 50% of clients from 40-60s followed by 60-80s, 20-40s and +80.\n",
    "* Sourcing Channel: 54% of clients from Channel A followed by B, C, D, E "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-variate: Box-Whisker Plots ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a box-whisker:    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Box-Whisker Plot ###\n",
    "* Age: mean age of renewed client group is higher than non-renewed group.\n",
    "* Premium: mean premiumd paid amout of renewed client group is higher than non-renewed group.\n",
    "* % Premiumd Paid by Cash Credit: non-renewed group has higher usage of cash credit than renewed group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-variate: Correlation Matrix Plot ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe:\n",
    "\n",
    "# check data type:\n",
    "\n",
    "# convert data type: object to category\n",
    "\n",
    "\n",
    "# encode categorical variables using specified numeric feature:\n",
    "\n",
    "\n",
    "# Create a list of variables:\n",
    "\n",
    "\n",
    "# Delete a list of unwanted variables:\n",
    "\n",
    "\n",
    "# plot a correlation matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Correlation Matrix Plot ###\n",
    "1. Renewal status shows negative moderate correlation with count premium paid 6-12 months late (r=-0.28)\n",
    "2. Renewal status shows negative weak correlation with % premium paid by cash credit (r=-0.24).\n",
    "3. Premium shows positive moderate correlation with income (r=0.30)\n",
    "4. Age shows negative weak correlation with % premium paid by cash credit (r=-0.26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-variate: Cross Tabulation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a cross-tabulation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Cross Tabulation ###\n",
    "1. renewal and age group: shows that age +80 group have the lowest # of clients in both groups. Majority of clients are from age 40-60s in both renewed and non-renewed client groups.\n",
    "2. renewal and sourcing channel: shows that channel E group has the lowest # of clients in both groups. Majority of clients are from Channel A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Analyses ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Plot a linear regression plot: w numerical variables ----#\n",
    "# Compute Pearson r for combination of X and Y:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a linear regression analysis:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Linear Regression ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
