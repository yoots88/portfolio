{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Stroke Risk Prediction - Exploratory Data Analysis #\n",
    "This is a capstone project for Springboard's data science intensive track. The dataset used in this project is sourced from the data science competition sponsor by McKinsey analytics and held in a platform \"Analytics Vidhya\". \n",
    "The competition link can be found here [contest page] (https://datahack.analyticsvidhya.com/contest/mckinsey-analytics-online-hackathon/).\n",
    "\n",
    "**Problem Statement:** a chain of hospitals in US collected anonymized pool of patients data for stroke classification. Stroke is one of critical disease which affects nearly 1 in 20 Americans and is a disease that affects arteries leading to and within the brain. A stroke occurs when a blood vessel that carries oxygen and nutrients to the brain is either blocked by a clot or ruptures. When that happens part of the brain cannot get the blood (and oxygen) it needs, so the brain cells in the affected region is dead. The clients captured several lifestyle, health monitoring measurements and demographic factors about their anonymized patients. These include features like age, gender, health monitoring measurements (i.e., glucose level, body mass index) and lifestyle factors (i.e., smoking status, occupation type etc.). \n",
    "\n",
    "**Stakeholders:**\n",
    "Hospital cardiac unit managers, a group of clinicians, insurance companies and employers from patient population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A. Import Cleaned Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoots\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import all libraries #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Authorization #\n",
    "__author__ = \"Taesun Yoo\"\n",
    "__email__ = \"yoots1988@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Write Out List of Functions --- #\n",
    "def load_file(file):\n",
    "    '''load the CSV files as a dataframe'''\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "def drop_column_by_index(df, var):\n",
    "    '''drop a column by specified variable'''\n",
    "    df = df.drop(var, axis=1)\n",
    "    return df\n",
    "\n",
    "def join_data(df_train, df_label, key, \n",
    "              left_index=None, right_index=None):\n",
    "    '''Merge the feature and label dataframe(s)'''\n",
    "    df_join = pd.merge(df_train, df_label, how='inner', on=key,\n",
    "                         left_index=False, right_index=False)\n",
    "    return df_join\n",
    "\n",
    "def clean_data(df):\n",
    "    '''drop any duplicate based on specific column'''\n",
    "    clean_df = df.drop_duplicates(subset='id')\n",
    "    return clean_df\n",
    "\n",
    "def eda_missing_data(df):\n",
    "    missing_df = pd.DataFrame(df.isnull().sum())\n",
    "    missing_df.columns = ['count']\n",
    "    missing_df['pct'] = (missing_df['count']/len(df))*100\n",
    "    return missing_df\n",
    "\n",
    "def eda_summary_stat_num(df):\n",
    "    '''compute summary statistics for numerical variables'''\n",
    "    df_stat_num = df.describe().T\n",
    "    df_stat_num = df_stat_num[['count', 'min', 'mean', 'max', '25%', '50%', '75%', 'std']]\n",
    "    df_stat_num = df_stat_num.sort_values(by='count', ascending=True)\n",
    "    df_stat_num = pd.DataFrame(df_stat_num)\n",
    "    return df_stat_num\n",
    "\n",
    "def eda_summary_stat_cat(df):\n",
    "    '''compute summary statistics for categorical variables'''\n",
    "    df_stat_cat = pd.DataFrame(df.describe(include='O').T)\n",
    "    return df_stat_cat\n",
    "\n",
    "def compute_outliers(df_stat_num):\n",
    "    df_stat_num['IQR'] = df_stat_num['75%'] - df_stat_num['25%']\n",
    "    df_stat_num['UB'] = df_stat_num['75%'] + 1.5*df_stat_num['IQR']\n",
    "    df_stat_num['LB'] = df_stat_num['25%'] - 1.5*df_stat_num['IQR']\n",
    "    df_outliers = df_stat_num[['LB', 'min', 'UB', 'max']]\n",
    "    return df_outliers\n",
    "\n",
    "def EDA_plot_correlation(df_EDA):\n",
    "    '''compute and plot correlation matrix'''\n",
    "    corr = df_EDA.corr()\n",
    "    # Create a mask to filter matrix: diagonally\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # Matrix Plot:\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    cmap = sns.diverging_palette(220,10,as_cmap=True)\n",
    "    sns.set(font_scale=1.1)\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                annot=True, square=True, linewidths=.5, fmt=\".2f\",\n",
    "                annot_kws={'size':10}, cbar_kws={'shrink':.6})\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "def encode_categorical_feature(df, var_name, map_name):\n",
    "    '''encode categorical features into mapping values'''\n",
    "    df[var_name] = df[var_name].map(map_name)\n",
    "    return df[var_name]\n",
    "\n",
    "def feature_imputer(X, missing_val_format, method, indices):\n",
    "    '''imputes missing values based on different uni-variate methods'''\n",
    "    imputer = Imputer(missing_values=missing_val_format, strategy=method, axis=0)\n",
    "    imputer = imputer.fit(X.iloc[:, indices])\n",
    "    X.iloc[:, indices] = imputer.transform(X.iloc[:, indices])\n",
    "    return X.iloc[:, indices]\n",
    "\n",
    "def convert_data_type(df, var_name, dt_type):\n",
    "    '''convert data type into specified metadata type'''\n",
    "    df[var_name] = df[var_name].astype(dt_type)\n",
    "    return df[var_name]\n",
    "\n",
    "def split_dataframe(df):\n",
    "    '''Split dataframe into features and label'''\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    return X, y\n",
    "\n",
    "def avg_groupby_data(df, num_var, cat_var, avg_var_name):\n",
    "    '''perform average group by categorical variable to compute a mean'''\n",
    "    avg_groupby_val = df.groupby(cat_var)[num_var].mean().sort_values(ascending=False)\n",
    "    avg_groupby_df = pd.DataFrame({cat_var:list(df[cat_var].unique()),\n",
    "                                   avg_var_name:avg_groupby_val})\n",
    "    avg_groupby_df.reset_index(drop=True, inplace=True)\n",
    "    return avg_groupby_df\n",
    "\n",
    "def left_join_data(train_df, avg_groupby_df, key=None, left_index=False, right_index=False):\n",
    "    '''performs left join on train data to average groupby data'''\n",
    "    joined_df = pd.merge(train_df, avg_groupby_df, how='left', on=key,\n",
    "                         left_index=left_index, right_index=right_index)\n",
    "    return joined_df\n",
    "\n",
    "def one_hot_encode_feature(df, cat_vars=None, num_vars=None):\n",
    "    '''performs one-hot encoding on all categorical variables and\n",
    "       combine results with numerical variables '''\n",
    "    cat_df = pd.get_dummies(df[cat_vars], drop_first=True)\n",
    "    num_df = df[num_vars].apply(pd.to_numeric)\n",
    "    return pd.concat([cat_df, num_df], axis=1)\n",
    "\n",
    "def get_label_data(df, label_var):\n",
    "    '''separate label from a dataframe'''\n",
    "    df_label = df[label_var]\n",
    "    return df_label\n",
    "\n",
    "def split_data_by_age_group(df, var_name):\n",
    "    '''split dataframe by age group'''\n",
    "    df_age_group = pd.DataFrame(df.groupby(var_name)[var_name].count().sort_values(ascending=False))\n",
    "    df_age_group.columns = ['count']\n",
    "    df_age_group.index.name = 'age_group'\n",
    "    return df_age_group\n",
    "\n",
    "def strata_by_age_group(df, group_name, idx):\n",
    "    '''stratify dataframe by label group index'''\n",
    "    df_strata = df[df[group_name] == idx]\n",
    "    return df_strata\n",
    "\n",
    "def resample_data_by_group(df, n_samples):\n",
    "    '''resample data by random replacement'''\n",
    "    sample_group = resample(df, n_samples=n_samples, random_state=0, replace=True)\n",
    "    return sample_group\n",
    "\n",
    "def EDA_feature_importance_plot(model, X, y):\n",
    "    '''plots the feature importance plot on trained model'''\n",
    "    model = model\n",
    "    model.fit(X, y)\n",
    "    feat_labels = X.columns\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "    plt.xticks(range(X.shape[1]), feat_labels[indices], rotation=90, fontsize=7)\n",
    "    plt.xlim(-1, X.shape[1])\n",
    "\n",
    "def feature_scale_data(X):\n",
    "    '''Feature scaled data based on standardization'''\n",
    "    sc_X = StandardScaler()\n",
    "    X_std = sc_X.fit_transform(X)\n",
    "    return X_std\n",
    "    \n",
    "# Plot confusion matrix: accuracy, precision, recall and etc.\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    '''plot the confusion matrix of trained model'''\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    cm = cm.astype('float')/cm.sum()\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt='.2f'\n",
    "    thresh = cm.max()/2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i,j], fmt), ha='center', va='center',\n",
    "                    color='white' if cm[i,j] > thresh else 'black')\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.ylabel('true label')\n",
    "\n",
    "# Write report classification metrics summary report\n",
    "def report_class_summary(model_name, y_act, y_pred):\n",
    "    print ('Accuracy of ' + model_name + ' is %0.2f'% skm.accuracy_score(y_act, y_pred))\n",
    "    print ('Precision of ' + model_name + ' is %0.2f'% skm.precision_score(y_act, y_pred))\n",
    "    print ('Recall of ' + model_name + ' is %0.2f'% skm.recall_score(y_act, y_pred))\n",
    "    print ('ROC score of ' + model_name + ' is %0.2f'% skm.roc_auc_score(y_act, y_pred))\n",
    "\n",
    "# Compute confusion matrix:\n",
    "def compute_confusion_matrix(y_act, y_pred):\n",
    "    '''compute sklearn confusion matrix'''\n",
    "    cm_model = skm.confusion_matrix(y_act, y_pred)\n",
    "    return cm_model    \n",
    "\n",
    "def score_model_roc_auc(model, X_train, y_train, X_val, y_val):\n",
    "    '''computes the roc_auc score for probability of being a stroke case'''\n",
    "    model.fit(X_train, y_train)\n",
    "    probs = model.predict_proba(X_val)\n",
    "    return skm.roc_auc_score(y_val, probs[:,1])\n",
    "\n",
    "def model_tuning_param(model, feature_df, label_df, param_dist, n_iter):\n",
    "    '''performs RandomizedSearchCV to tune model hyper-parameters'''\n",
    "    random_search = RandomizedSearchCV(model, param_dist, n_iter, cv=5)\n",
    "    random_search.fit(feature_df, label_df)\n",
    "    return random_search\n",
    "\n",
    "def print_best_param(random_search, param_1=None, param_2=None, param_3=None, param_4=None):\n",
    "    '''print the best model parameter(s)'''\n",
    "    print(\"Best \" + param_1 + \":\", random_search.best_estimator_.get_params()[param_1])\n",
    "    print(\"Best \" + param_2 + \":\", random_search.best_estimator_.get_params()[param_2])\n",
    "    print(\"Best \" + param_3 + \":\", random_search.best_estimator_.get_params()[param_3])\n",
    "    print(\"Best \" + param_4 + \":\", random_search.best_estimator_.get_params()[param_4])\n",
    "\n",
    "def model_train(model, feature_df, label_df, n_proc, mean_roc_auc, cv_std):\n",
    "    '''train a model and output mean roc_auc and CV std.dev roc_auc'''\n",
    "    roc_auc = cross_val_score(model, feature_df, label_df, n_jobs=n_proc,\n",
    "                               cv=5, scoring='roc_auc')\n",
    "    mean_roc_auc[model] = np.mean(roc_auc)\n",
    "    cv_std[model] = np.std(roc_auc)    \n",
    "\n",
    "def model_summary(model, mean_roc_auc, cv_std):\n",
    "    '''print out the model performances'''\n",
    "    print('\\nModel:\\n', model)\n",
    "    print('Average roc_auc:\\n', mean_roc_auc[model])\n",
    "    print('Std. Dev during CV:\\n', cv_std[model])    \n",
    "\n",
    "def model_results(model, mean_roc_auc, predictions, feature_importances):\n",
    "    '''saves the model name, mean_roc_auc, predicted rate, and feature importances'''\n",
    "    with open('model.txt', 'w') as file:\n",
    "        file.write(str(model))\n",
    "        feature_importances.to_csv('feat_importances.csv')\n",
    "        predictions.to_csv('pred_results_best.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43400 entries, 0 to 43399\n",
      "Data columns (total 12 columns):\n",
      "id                   43400 non-null int64\n",
      "gender               43400 non-null object\n",
      "age                  43400 non-null float64\n",
      "hypertension         43400 non-null int64\n",
      "heart_disease        43400 non-null int64\n",
      "ever_married         43400 non-null object\n",
      "work_type            43400 non-null object\n",
      "Residence_type       43400 non-null object\n",
      "avg_glucose_level    43400 non-null float64\n",
      "bmi                  41938 non-null float64\n",
      "smoking_status       30108 non-null object\n",
      "stroke               43400 non-null int64\n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Load the data --- #\n",
    "if __name__ == '__main__':\n",
    "# Define input CSVs:\n",
    "    train_file = 'stroke_train.csv'\n",
    "    test_file = 'stroke_test.csv'\n",
    "\n",
    "# Define type of variables list:\n",
    "#df_train.select_dtypes(include='object').columns\n",
    "cat_vars = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "#df_train.select_dtypes(include='int64').columns\n",
    "#df_train.select_dtypes(include='float64').columns\n",
    "num_vars = ['hypertension', 'heart_disease', 'age', 'avg_glucose_level', 'bmi']\n",
    "label_var = 'stroke'\n",
    "\n",
    "# Define variables to drop\n",
    "list_vars = 'id'\n",
    "\n",
    "# Load data\n",
    "df_train = load_file(train_file)\n",
    "df_test = load_file(test_file)\n",
    "\n",
    "# Check the metadata of dataframe:\n",
    "df_train.info()\n",
    "\n",
    "# Create a label dataframe:\n",
    "df_label = df_train[['id', 'stroke']]\n",
    "\n",
    "# Drop a column by index: poverty_rate\n",
    "df_train = drop_column_by_index(df_train, label_var)\n",
    "\n",
    "# join train set and label:\n",
    "train_raw_df = join_data(df_train, df_label, key='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Exploratory Data Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stroke Patients Data: training set** \n",
    "\n",
    "Data exploration is conducted on a cleaned training set. The main goal of this phase is to explore any interesting relationships among features and identify which features are good predictors on stroke label predictions.\n",
    "\n",
    "Following set of questions are asked:\n",
    "\n",
    "Can I count something interesting?\n",
    "Can I find some trends (increase or decrease and any anomalies)?\n",
    "Can I plot a bar chart or a histogram?\n",
    "Can I make a scatter plot?\n",
    "These set of guiding questions will help us to explore any insights and tell a compelling story about the US poverty dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
